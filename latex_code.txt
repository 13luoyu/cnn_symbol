\documentclass[11pt]{article}
\usepackage{CJK}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
 
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 
\begin{document}
\begin{CJK*}{UTF8}{gkai}
%SetUp函数
 
    \begin{algorithm}
        \caption{Symbol Propagation in Convolutional Layer}
        \begin{algorithmic}[1] %每行显示行号
        \Require {Symbol input $x$, the convolution layer parameters including kernel $w$, bias $b$, padding $p$ and stride $s$}
        \Ensure {Symbol output $y$}
            \Function {ConvPropagation}{$x$,$w$,$b$,$p$,$s$}
                
                \State $y_h\gets (x_h + p_h * 2 - w_h) / s_h + 1$;
                \State $y_w\gets (x_w + p_w * 2 - w_w) / s_w + 1$;
                \State Initialize $y$ with shape ($w_o$, $y_h$, $y_w$) to value 0;
                \State Compute the padding of $x$ as $x_p$ with input $x$ and padding $p$;
                
                %forall
                \ForAll {output channels}  
                    \ForAll{input channels}
                        \ForAll{output height $y_h$}
                            \State Compute the corresponding begin and end height index in $x_p$;
                            \ForAll{output width $y_w$}
                                \State Compute the corresponding begin and end width index in $x_p$;
                                \State Initialize $local_y$ as dictionary and initialize it to 0;
                                \State Compute $local_y$ with symbol convolution multiplication for every input $x_p$ between begin and end height and width, which for $xi$, $wi$ in x_p, $local_y$ += $wi$ * $w$, where $xi$ is symbol, $wi$ is its weight and $w$ is the convolution kernel value;
                                \State Set the corresponding position of $y$ to $local_y$ if $y$=0, else add them together;
                            \EndFor
                        \EndFor
                    \EndFor
                    \State Add bias $b$ to every channel of output $y$;
                \EndFor
                \State
                \Return $y$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    \begin{algorithm}
        \caption{Symbol Propagation in Fully Connected Layer}
        \begin{algorithmic}[1] %每行显示行号
        \Require {Symbol input $x$, weight $w$, bias $b$}
        \Ensure {Symbol output $y$}
            \Function {LinearPropagation}{$x$,$w$,$b$}
                
                \State Initialize $y$ with shape ($w_o$) to value 0;
                
                %forall
                \ForAll {output channels}
                    Initialize $local_y$ as dictionary and initialize it to 0;
                    \ForAll{input channels}
                        Compute $local_y$ with symbol matmul multiplication for every input $x$, which for $xi$, $wi$ in $x$, $local_y$[xi] += $wi$ * $w$, $xi$ is the symbol, $wi$ is the weight of $xi$ and $w$ is the weight of fully connected layer;
                    \EndFor
                    \State Add bias $b$ to $local_y$;
                    \State Append $local_y$ to the end of $y$;
                \EndFor
                \State
                \Return $y$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    
    \begin{algorithm}
        \caption{Symbol Propagation in Activation Function Layer}
        \begin{algorithmic}[1] %每行显示行号
        \Require {Symbol input $x$, activation function $f$, interval input value $x_l$, $x_h$}
        \Ensure {Symbol output $y$, interval output value $y_l$, $y_h$}
            \Function {ActivationPropagation}{$x$,$f$,$x_l$,$x_h$}
                
                \State Initialize $y_l$, $y_h$ with shape $x$ to value 0;
                \State Initialize $y$ with shape $x$ to empty dictionary;
                
                %forall
                \ForAll {dimension of input $x$}
                    \State $sum_l\gets 0.0$;
                    \State $sum_h\gets 0.0$;
                    \ForAll{symbol $xi$ and its weight $wi$ in dictionary x}
                        \If{$xi$ is the symbol of bias}
                            \State $sum_l\gets sum_l + wi$;
                            \State $sum_h\gets sum_h + wi$;
                        \Else
                            \State Get the true value interval of $xi$ as $local_l$ and $local_h$;
                            \If{$wi$ > 0}
                                \State $sum_l\gets sum_l + wi * local_l$;
                                \State $sum_h\gets sum_h + wi * local_h$;
                            \Else
                                \State $sum_l\gets sum_l + wi * local_h$;
                                \State $sum_h\gets sum_h + wi * local_l$;
                            \EndIf
                        \EndIf
                        \State Set the corresponding position value of $y_l$ to $f$($sum_l$), set the corresponding position value of $y_h$ to $f$($sum_h$), where $f$ is the activation function;
                        \State Set the corresponding position value of $y$ as \{'xi':1\}, where $xi$ is the new symbol and 1 is its initial weight;
                    \EndFor
                \EndFor
                \State
                \Return $y$, $y_l$, $y_h$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    
    \begin{algorithm}
        \caption{Symbol Propagation in Batch Normalization Layer}
        \begin{algorithmic}[1] %每行显示行号
        \Require {Symbol input $x$, the mean of train batch inputs $mean$, the variance of train batch inputs $var$, the weight $gamma$, the bias $beta$ and a minimal value $eps$ to avoid zero division of this layer}
        \Ensure {Symbol output $y$}
            \Function {LinearPropagation}{$x$,$mean$,$var$,$gamma$,$beta$,$eps$}
                
                \State Initialize $y$ with the shape of $x$ as a dictionary;
                
                %forall
                \ForAll {dimension of input $x$}
                    \ForAll{symbol $xi$ and its weight $wi$ in dictionary $x$}
                        \State $y[xi]\gets wi * gamma / \sqrt{var + eps}$;
                    \EndFor
                    \State $b\gets -mean * gamma / \sqrt{var + eps} + beta$;
                    \State Set the weight of bias symbol of $y$ to ($x_b$-$b$) where $x_b$ is the weight of bias symbol of $x$;
                \EndFor
                \State
                \Return $y$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    
    \begin{algorithm}
        \caption{Symbol Propagation in Max Pooling Layer}
        \begin{algorithmic}[1] %每行显示行号
        \Require {Symbol input $x$, the size of pooling window $kernel\_size$, padding $p$, stride $s$, the lower and upper bound of $x$ $x_l$, $x_h$}
        \Ensure {Symbol output $y$, the lower and upper bound of $y$ $y_l$, $y_h$}
            \Function {LinearPropagation}{$x$,$kernel_size$,$p$,$s$,$x_l$,$x_h$}
                
                \State $y_h\gets (x_h + p_h * 2 - w_h) / s_h + 1$;
                \State $y_w\gets (x_w + p_w * 2 - w_w) / s_w + 1$;
                \State Initialize $y$ with shape ($x_c$, $y_h$, $y_w$) to empty dictionary;
                \State Initialize $y_l$, $y_h$ with the shape of $y$ to value 0;
                \State Compute the pad of $x$ $x_p$ with input $x$ and padding $p$;
                
                %forall
                \ForAll {input channels}
                    \ForAll{point of output $y$}
                        \State Compute the corresponding area $a$ of input $x$;
                        \State $local_l\gets +\infty$;
                        \State $local_h\gets -\infty$;
                        \ForAll{point $p$ in area $a$}
                            \If{$p$ is not a padding point}
                                \State Compute the true upper bound and lower bound of x as $xb_l$, $xb_h$; 
                                \State $local_l\gets max(local_l, xb_l)$;
                                \State $local_h\gets max(local_h, xb_h)$;
                            \Else
                                \State Do nothing;
                            \EndIf
                        \EndFor
                        \State Set the corresponding position of $y$ to $y[xi]\gets 1$ where $xi$ is the new symbol and 1 is its weight;
                        \State Set the corresponding position of $y_l$, $y_h$ to value $local_l$, $local_h$
                    \EndFor
                \EndFor
                \State
                \Return $y$, $y_l$, $y_h$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    
    \begin{algorithm}
        \caption{Symbol Propagation in Average Pooling Layer}
        \begin{algorithmic}[1] %每行显示行号
        \Require {Symbol input $x$, the size of pooling window $kernel\_size$, padding $p$, stride $s$}
        \Ensure {Symbol output $y$}
            \Function {LinearPropagation}{$x$,$kernel_size$,$p$,$s$,$x_l$,$x_h$}
                
                \State $y_h\gets (x_h + p_h * 2 - w_h) / s_h + 1$;
                \State $y_w\gets (x_w + p_w * 2 - w_w) / s_w + 1$;
                \State Initialize $y$ with shape ($x_c$, $y_h$, $y_w$) to empty dictionary;
                \State Compute the pad of $x$ $x_p$ with input $x$ and padding $p$;
                
                %forall
                \ForAll {input channels}
                    \ForAll{point of output $y$}
                        \State Compute the corresponding area $a$ of input $x$;
                        \ForAll{point $p$ in area $a$}
                            \If{$p$ is not a padding point}
                                \State For every symbol $xi$ and its weight $wi$ in $p$, the corresponding output $y[xi]$ += $wi$;
                            \Else
                                \State Do nothing;
                            \EndIf
                        \EndFor
                    \EndFor
                \EndFor
                \State Calculate the number of elements in pooling window as $div$;
                \ForAll{dimension of $y$}:
                    \ForAll{$xi$, $wi$ in $y$}
                        $y[xi]\gets wi / div$;
                    \EndFor
                \EndFor
                \State
                \Return $y$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    
    
    
    \begin{algorithm}
        \caption{Interval Compare Algorithm}
        \begin{algorithmic}[1] %每行显示行号
        \Require {label\_list $l$, true\_label $tl$, output\_interval $interval$}
        \Ensure {wrong\_label\_list $wl$}
            \Function {IntervalCompare}{$l$,$tl$,$interval$}
                
                \State Initialize wrong label list: $wl\gets []$;
                
                \ForAll{$label$ in $l$}
                    \State $i1\gets interval[tl]$;
                    \State $i2\gets interval[label]$;
                    \State Assume $i1$ = [$a$,$b$], i2=[$c$,$d$];
                    \If{$a$ \textless $d$}
                        \State Add $label$ to $wl$;
                    \EndIf
                \EndFor
                \State
                \Return $wl$
                
                
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    
    
    
    
    
    \begin{algorithm}
        \caption{Reverse Symbol Propagation in Activation Function Layer}
        \begin{algorithmic}[1] %每行显示行号
        \Require {the index of symbol hope to be min $want\_min$, the index of symbol hope to be max $want\_max$, the weight of want\_min and want\_max $weight\_dic$, input symbol expression $x$, }
        \Ensure {new symbol hope to be min $want\_min$, new symbol hope to be max $want\_max$, the new weight of want\_min and want\_max $new\_weight\_dic$}
            \Function {ActReversePropagation}{$want\_min$, $want\_max$, $weight\_dic$, $x$}
                
                \State $new\_want\_min\gets []$;
                \State $new\_want\_max\gets []$;
                \State Initialize $want\_min\_weight\_dic$, $want\_max\_weight\_dic$ as dictionary;
                
                    \ForAll{$xii$ in $want\_min$}
                        \State Calculate the index of xii, then get its input express from x as $express$;
                        \ForAll{$xi$, $wi$ in $express$}
                            \If{$xi$ is the symbol of bias}
                                \State continue;
                            \EndIf
                            \If{$wi$ \textless 0}
                                \State Add $xi$ to $new\_want\_max$;
                                \State $want\_max\_weight\_dic$[$xi$] += abs(wi) * $weight\_dic$[$xii$];
                            \Else
                                \State Add $xi$ to $new\_want\_min$;
                                \State $want\_min\_weight\_dic$[$xi$] += wi * $weight\_dic$[$xii$];
                            \EndIf
                        \EndFor
                    \EndFor
                
                \State Do the same thing to list $want\_max$ just as below;
                
                \State $want\_min\gets []$;
                \State $want\_max\gets []$;
                \State Initialize $new\_want\_dic$ as dictionary;
                \ForAll{$xi$ in $new\_want\_max$}
                    \If{$xi$ in $new\_want\_min$ and $new\_want\_max$}
                        \State If the weight of min is larger, add $xi$ to $want\_min$, else add it to $want\_max$, set $new\_want\_dic$ of $xi$ to the difference of the two weight below;
                    \Else
                        \State set $xi$ to where it want, and set its $new\_want\_dic$ to its weight;
                    \EndIf
                \EndFor
                
                \State
                \Return $want\_min$, $want\_max$, $new\_want\_dic$;
                
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    
    
\end{CJK*}
\end{document}